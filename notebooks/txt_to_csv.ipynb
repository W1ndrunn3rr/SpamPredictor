{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df1c31d-f74f-49f9-89ec-5c13317cd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import asent\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6f0045-139f-493c-8a74-e6e20a7bb0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<asent.component.Asent at 0x7fe525f87aa0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(\"asent_en_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47884ede-a735-4a4a-988c-d0904661fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def ProcessText(content):\n",
    "        wordDict = {\"NOUN\": 0, \"PROPN\": 0, \"VERB\": 0, \"ADJ\": 0, \"ADV\": 0}\n",
    "        uniqueTokens = set()\n",
    "        sentimentalScore = None\n",
    "        tokens = 0\n",
    "\n",
    "        doc = nlp(content)\n",
    "        tokens = [\n",
    "            token.text.lower() for token in doc if token.pos_ in  wordDict.keys()\n",
    "        ]\n",
    "\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "\n",
    "     \n",
    "        for token in doc:\n",
    "            if token.pos_ in wordDict.keys():\n",
    "                wordDict[token.pos_] += 1\n",
    "\n",
    "        tokens_n = len(tokens)\n",
    "\n",
    "        uniqueTokens.update(tokens)\n",
    "\n",
    "        sentimentalScore = doc._.polarity\n",
    "\n",
    "        return wordDict, uniqueTokens, sentimentalScore, tokens_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072aecf7-289e-462d-8683-7caa7adab60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac02845c-b70a-450d-8b84-11ba051bc9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PrepareSingleDataText(title,wordDict, uniqueTokens, sentimentalScore, tokens, label):\n",
    "        global counter\n",
    "        total_w = wordDict[\"NOUN\"] + wordDict[\"PROPN\"] + wordDict[\"VERB\"] + wordDict[\"ADV\"] + wordDict[\"ADJ\"]\n",
    "        dataframe = {\n",
    "            \"title\" : title,\n",
    "            \"neg_score\": round(sentimentalScore.negative,3),\n",
    "            \"neu_score\": round(sentimentalScore.neutral,3),\n",
    "            \"pos_score\": round(sentimentalScore.positive,3),\n",
    "            \"compound_score\": round(sentimentalScore.compound,3),\n",
    "            \"n_sentences\": sentimentalScore.n_sentences,\n",
    "            \"n_tokens\": tokens,\n",
    "            \"unique_tokens_r\": round(len(uniqueTokens) / tokens,3),\n",
    "            \"nouns_r\": round(wordDict[\"NOUN\"] / total_w,3),\n",
    "            \"proper_nouns_r\": round(wordDict[\"PROPN\"] / total_w,3),\n",
    "            \"verbs_r\": round(wordDict[\"VERB\"] / total_w,3),\n",
    "            \"adverbs_r\": round(wordDict[\"ADV\"] / total_w,3),\n",
    "            \"adjectives_r\": round(wordDict[\"ADJ\"] / total_w,3),\n",
    "            \"news\" : label\n",
    "        }\n",
    "        return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0153e86e-bbd6-4ca5-84ed-bdc8562a5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "\n",
    "def ProcessChunk(texts, label):\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        try:\n",
    "            wordDict, uniqueTokens, sentimentalScore, tokens_n = ProcessText(text)\n",
    "            data = PrepareSingleDataText(text[:50] + \"...\", wordDict,uniqueTokens, sentimentalScore, tokens_n, label)\n",
    "            results.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {text[:50]}... Error: {str(e)}\")\n",
    "    return results\n",
    "\n",
    "def CreateDatasetFromFile(file_path_1, file_path_2):\n",
    "    with open(file_path_1, \"r\") as f:\n",
    "        texts_1 = [t.strip() for t in f.read().split(\"\\n\") if t.strip()]\n",
    "    with open(file_path_2, \"r\") as f:\n",
    "        texts_2 = [t.strip() for t in f.read().split(\"\\n\") if t.strip()]\n",
    "\n",
    "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "        chunk_size = 1000  # Dostosuj tę wartość\n",
    "        real_chunks = [texts_1[i:i+chunk_size] for i in range(0, len(texts_1), chunk_size)]\n",
    "        fake_chunks = [texts_2[i:i+chunk_size] for i in range(0, len(texts_2), chunk_size)]\n",
    "\n",
    "        real_results = pool.starmap(process_chunk, [(chunk, \"Real\") for chunk in real_chunks])\n",
    "        fake_results = pool.starmap(process_chunk, [(chunk, \"Fake\") for chunk in fake_chunks])\n",
    "\n",
    "    all_data = [item for sublist in real_results + fake_results for item in sublist]\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.set_index(\"title\", inplace=True)\n",
    "    df.to_csv(\"data.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff5055b0-a1ce-42d3-b200-51cf2cc3d70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing text: and... Error: division by zero\n",
      "Error processing text: and... Error: division by zero\n",
      "Error processing text: 2016... Error: division by zero\n",
      "Error processing text: 48413... Error: division by zero\n",
      "Error processing text: 0 12... Error: division by zero\n",
      "Error processing text: 65975... Error: division by zero\n",
      "Error processing text: by 2016... Error: division by zero\n",
      "Error processing text: In... Error: division by zero\n",
      "Error processing text: for... Error: division by zero\n",
      "Error processing text: 221... Error: division by zero\n",
      "Error processing text: f... Error: division by zero\n",
      "Error processing text: and m... Error: division by zero\n",
      "Error processing text: 2016 by by... Error: division by zero\n",
      "Error processing text: 66.1... Error: division by zero\n",
      "Error processing text: 6416... Error: division by zero\n",
      "Error processing text: 2016 by... Error: division by zero\n",
      "Error processing text: up... Error: division by zero\n",
      "Error processing text: no... Error: division by zero\n",
      "Error processing text: 2014 64... Error: division by zero\n",
      "Error processing text: 0 2016 0 2016 0... Error: division by zero\n",
      "Error processing text: A 2 2016 2014 2011 SO 2016 SO bu ad bu... Error: division by zero\n",
      "Error processing text: la en LA... Error: division by zero\n",
      "Error processing text: About the and . to pm on 2016... Error: division by zero\n",
      "Error processing text: 2016 in... Error: division by zero\n",
      "Error processing text: 231... Error: division by zero\n",
      "Error processing text: By... Error: division by zero\n",
      "Error processing text: 9315... Error: division by zero\n",
      "Error processing text: Will in 2017... Error: division by zero\n",
      "Error processing text: He is... Error: division by zero\n",
      "Error processing text: That's a... Error: division by zero\n",
      "Error processing text: By... Error: division by zero\n",
      "Error processing text: her 75.7... Error: division by zero\n",
      "Error processing text: out of out of the... Error: division by zero\n",
      "Error processing text: 94243... Error: division by zero\n",
      "Error processing text: ck the K ha ha ha... Error: division by zero\n",
      "Error processing text: 34478... Error: division by zero\n"
     ]
    }
   ],
   "source": [
    "CreateDatasetFromFile(\"../data/real.txt\", \"../data/fake.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30ef645-fbb8-40a8-9a76-3baf0004528d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_test\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a84f6-ba6a-4d6e-a932-def7ad316086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
